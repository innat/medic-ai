{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Medicai","text":"<p>Medic-AI is a Keras based library designed for medical image analysis using machine learning techniques. It provides seamless compatibility with multiple backends, allowing models to run on <code>tensorflow</code>, <code>torch</code>, and <code>jax</code>.</p> <p>Note: It is currently in its early stages and will undergo multiple iterations before reaching a stable release.</p>"},{"location":"#installation","title":"Installation","text":"<pre><code>git clone https://github.com/innat/medic-ai\ncd medic-ai\npip install . -q\n</code></pre>"},{"location":"#available-features","title":"Available Features","text":"<p>The <code>medicai</code> library provides a range of features for medical image processing, model training, and inference. Below is an overview of its key functionalities.</p> <p>Image Transformations</p> <p><code>medicai</code> includes various transformation utilities for preprocessing medical images:</p> <ul> <li>Basic Transformations:<ul> <li><code>Resize</code> \u2013 Adjusts the image dimensions.</li> <li><code>ScaleIntensityRange</code> \u2013 Normalizes intensity values within a specified range.</li> <li><code>CropForeground</code> \u2013 Crops the image to focus on the region of interest.</li> <li><code>Spacing</code> \u2013 Resamples the image to a target voxel spacing.</li> <li><code>Orientation</code> \u2013 Standardizes image orientation.</li> </ul> </li> <li>Augmentations for Robustness:<ul> <li><code>RandRotate90</code> \u2013 Randomly rotates images by 90 degrees.</li> <li><code>RandShiftIntensity</code> \u2013 Randomly shifts intensity values.</li> <li><code>RandFlip</code> \u2013 Randomly flips images along specified axes.</li> </ul> </li> <li>Pipeline Composition:<ul> <li><code>Compose</code> \u2013 Chains multiple transformations into a single pipeline.</li> </ul> </li> </ul> <p>Models</p> <p>Currently, <code>medicai</code> focuses on 3D models for classification and segmentation:</p> <ul> <li><code>SwinTransformer</code> \u2013 3D classification task.</li> <li><code>SwinUNETR</code> \u2013 3D segmentation task.</li> </ul> <p>Inference</p> <ul> <li><code>SlidingWindowInference</code> \u2013 Processes large 3D images in smaller overlapping windows, improving performance and memory efficiency.</li> </ul>"},{"location":"#guides-wip","title":"Guides (WIP)","text":"<ul> <li>3D transformation</li> <li>3D classification</li> <li>3D Segmentation</li> </ul>"},{"location":"#acknowledgements","title":"Acknowledgements","text":"<p>This project is greatly inspired by MONAI.</p>"},{"location":"#citation","title":"Citation","text":"<p>If you use <code>medicai</code> in your research, please cite it using the metadata from our <code>CITATION.cff</code> file.</p>"},{"location":"general/","title":"General","text":""},{"location":"general/#generate-3d-nii-to-tfrecord-dataset","title":"Generate 3D <code>.nii</code> to TFRecord Dataset","text":"<p>The process is shown here.</p>"},{"location":"models/manage-models/","title":"Model","text":"<p>Currently only two models are implemented for 3D classification and segmentation task. The workflow can be run with <code>tensorflow</code>, and <code>torch</code> backend.</p>"},{"location":"models/manage-models/#3d","title":"3D","text":"<p>Classification</p> <pre><code>import tensorflow as tf\nfrom medicai.models import SwinTransformer\n\nnum_classes = 4\ninput_shape = (96, 96, 96, 1)\nmodel = SwinTransformer(\n    input_shape=input_shape, \n    num_classes=num_classes, \n    classifier_activation=None\n)\n\ndummy_input = tf.random.normal((1, 96, 96, 96, 1))\noutput = model(dummy_input)\noutput.shape\nTensorShape([1, 4])\n</code></pre> <p>Segmentation</p> <pre><code>import tensorflow as tf\nfrom medicai.models import SwinUNETR\n\nnum_classes = 4\ninput_shape = (96, 96, 96, 1)\nmodel = SwinUNETR(\n    input_shape=input_shape, \n    num_classes=num_classes,\n    classifier_activation=None\n)\n\ndummy_input = tf.random.normal((1, 96, 96, 96, 1))\noutput = model(dummy_input)\noutput.shape\nTensorShape([1, 96, 96, 96, 4])\n</code></pre>"},{"location":"training/manage-training/","title":"Training","text":""},{"location":"training/manage-training/#classification","title":"Classification","text":"<p>A high-level overview of the 3D classification process using <code>medicai</code>.</p> <pre><code>import os\nos.environ[\"KERAS_BACKEND\"] = \"tensorflow\" # tensorflow, torch\n\nimport keras\n\nfrom medicai.models import SwinTransformer\nfrom medicai.transforms import (\n    Compose,\n    ScaleIntensityRange,\n    RandRotate90,\n    Resize\n)\n</code></pre> <p>Transformation</p> <p>Import processing and augmentation operations for training, while using only processing operations for validation.</p> <pre><code>def train_transformation(image, label):\n    data = {\"image\": image, \"label\": label}\n    pipeline = Compose(\n        [\n            ScaleIntensityRange(\n                keys=[\"image\"], a_min=-175, a_max=250, b_min=0.0, b_max=1.0, clip=True\n            ),\n            Resize(keys=[\"image\"], mode=['bilinear'], spatial_shape=(96,96,96)),\n            RandRotate90(keys=[\"image\"], prob=0.1, max_k=3, spatial_axes=(1, 2))\n        ]\n    )\n    result = pipeline(data)\n    return result.data[\"image\"], result.data[\"label\"]\n\ndef val_transformation(image, label):\n    data = {\"image\": image, \"label\": label}\n    pipeline = Compose(\n        [\n            ScaleIntensityRange(\n                keys=[\"image\"], a_min=-175, a_max=250, b_min=0.0, b_max=1.0, clip=True\n            ),\n            Resize(keys=[\"image\"], mode=['bilinear'], spatial_shape=(96,96,96)),\n        ]\n    )\n    result = pipeline(data)\n    return result.data[\"image\"], result.data[\"label\"]\n</code></pre> <p>Dataloader</p> <p>Let's build the dataloader using <code>keras.utils.PyDataset</code>.</p> <pre><code>import numpy as np\nimport nibabel as nib\n\nclass NiftiDataLoader(keras.utils.PyDataset):\n    def __init__(\n        self, \n        image_paths, \n        labels,\n        batch_size=1, \n        dim=(128, 128, 128), \n        shuffle=True, \n        training=True\n    ):\n        self.image_paths = image_paths\n        self.labels = labels\n        self.batch_size = batch_size\n        self.dim = dim\n        self.shuffle = shuffle\n        self.training = training\n        self.on_epoch_end()\n\n    def __len__(self):\n        return int(np.floor(len(self.image_paths) / self.batch_size))\n\n    def __getitem__(self, index):\n        indices = self.indices[index * self.batch_size:(index + 1) * self.batch_size]\n        image_paths_batch = [self.image_paths[k] for k in indices]\n        labels_batch = [self.labels[k] for k in indices]\n\n        X = np.zeros((self.batch_size, *self.dim, 1), dtype=np.float32)\n        y = np.zeros((self.batch_size), dtype=np.float32)\n\n        for i, (img_path, label) in enumerate(zip(image_paths_batch, labels_batch)):\n            # Load and preprocess image\n            img = nib.load(img_path).get_fdata()\n\n            # Add channel dimension if needed\n            if img.ndim == 3:\n                img = np.expand_dims(img, axis=-1)\n\n            if self.training:\n                img, label = train_transformation(img, label)\n            else:\n                img, label = val_transformation(img, label)\n\n            X[i] = img\n            y[i] = label\n\n        return X, y\n\n    def on_epoch_end(self):\n        self.indices = np.arange(len(self.image_paths))\n        if self.shuffle:\n            np.random.shuffle(self.indices)\n\n\ntrain_loader = NiftiDataLoader(\n    image_paths=X_train,\n    labels=y_train,\n    batch_size=3,\n    dim=(96, 96, 96),\n    shuffle=True,\n    training=True\n)\n\nval_loader = NiftiDataLoader(\n    image_paths=X_test,\n    labels=y_test,\n    batch_size=3,\n    dim=(96, 96, 96),\n    shuffle=False,\n    training=False\n)\n</code></pre> <p>Model</p> <p>Create the model and compile it with the necessary loss function and metrics.</p> <pre><code>model = SwinTransformer(\n    input_shape=(96, 96, 96, 1),\n    num_classes=1,\n    classifier_activation='sigmoid',\n)\nmodel.compile(\n    optimizer=keras.optimizers.Adam(\n        learning_rate=1e-4,\n    ),\n    loss=keras.losses.BinaryCrossentropy(from_logits=False),\n    metrics=[\"acc\"],\n    jit_compile=False,\n)\n</code></pre> <p>Training</p> <pre><code>history = model.fit(\n    train_loader, \n    epochs=10,\n    validation_data=val_loader\n)\n</code></pre>"},{"location":"training/manage-training/#segmentation","title":"Segmentation","text":"<p>A high-level overview of the 3D segmentation process using <code>medicai</code>.</p> <pre><code>import keras\nimport tensorflow as tf\nfrom medicai.metrics import DiceMetric\nfrom medicai.losses import SparseDiceCELoss\nfrom medicai.models import SwinUNETR\nfrom medicai.transforms import (\n    Compose,\n    ScaleIntensityRange,\n    CropForeground,\n    RandCropByPosNegLabel,\n    Spacing,\n    Orientation,\n    RandShiftIntensity,\n    RandRotate90,\n    RandFlip\n)\nfrom medicai.callbacks import SlidingWindowInferenceCallback\n</code></pre> <p>Transformation</p> <p>Import processing and augmentation operations for training, while using only processing operations for validation.</p> <pre><code>def train_transformation(sample):\n    meta = {\"affine\": sample[\"image_affine\"]} # Since image and label affine are the same\n    data = {\"image\": sample[\"image\"], \"label\": sample[\"label\"]}\n    pipeline = Compose([\n        ScaleIntensityRange(\n            keys=[\"image\"],\n            a_min=-175,\n            a_max=250,\n            b_min=0.0,\n            b_max=1.0,\n            clip=True\n        ),\n        CropForeground(\n            keys=(\"image\", \"label\"),\n            source_key=\"image\"\n        ),\n        Orientation(keys = (\"image\", \"label\"), axcodes = \"RAS\"),\n        Spacing(pixdim=(2.0, 1.5, 1.5), keys=[\"image\", \"label\"]),\n        RandCropByPosNegLabel(\n            keys=(\"image\", \"label\"),\n            spatial_size=[96, 96, 96], \n            pos=1, \n            neg=1, \n            num_samples=1\n        ),\n        RandFlip(keys=[\"image\", \"label\"], spatial_axis=[0], prob=0.1),\n        RandFlip(keys=[\"image\", \"label\"], spatial_axis=[1], prob=0.1),\n        RandFlip(keys=[\"image\", \"label\"], spatial_axis=[2], prob=0.1),\n        RandRotate90(keys=[\"image\", \"label\"], prob=0.1, max_k=3, spatial_axes=(0, 1)),\n        RandShiftIntensity(keys=[\"image\"], offsets=0.10, prob=0.50)\n    ])\n    result = pipeline(data, meta)\n    return result.data[\"image\"], result.data[\"label\"]\n\n\ndef val_transformation(sample):\n    meta = {\"affine\": sample[\"image_affine\"]} # Since image and label affine are the same\n    data = {\"image\": sample[\"image\"], \"label\": sample[\"label\"]}\n    pipeline = Compose([\n        ScaleIntensityRange(\n            keys=[\"image\"],\n            a_min=-175,\n            a_max=250,\n            b_min=0.0,\n            b_max=1.0,\n            clip=True\n        ),\n        CropForeground(\n            keys=(\"image\", \"label\"),\n            source_key=\"image\"\n        ),\n        Orientation(keys = (\"image\", \"label\"), axcodes = \"RAS\"),\n        Spacing(pixdim=(2.0, 1.5, 1.5), keys=[\"image\", \"label\"])\n    ])\n    result = pipeline(data, meta)\n    return result.data[\"image\"], result.data[\"label\"]\n</code></pre> <p>Dataloader</p> <p>Create the dataloader using <code>tf.data.TFRecordDataset</code> API. Generating <code>tfrecod</code> is shown here</p> <pre><code>def load_tfrecord_dataset(tfrecord_pattern, batch_size=1, shuffle=True):\n    dataset = tf.data.TFRecordDataset(tf.io.gfile.glob(tfrecord_pattern))\n    dataset = dataset.shuffle(buffer_size=50) if shuffle else dataset\n    dataset = dataset.map(parse_tfrecord_fn, num_parallel_calls=tf.data.AUTOTUNE)\n    dataset = dataset.map(rearrange_shape, num_parallel_calls=tf.data.AUTOTUNE)\n    if shuffle:\n        dataset = dataset.map(train_transformation, num_parallel_calls=tf.data.AUTOTUNE)\n    else:\n        dataset = dataset.map(val_transformation, num_parallel_calls=tf.data.AUTOTUNE)\n    dataset = dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n    return dataset\n\n\ntfrecord_pattern = \"1/tfrecords/{}_shard_*.tfrec\"\n\ntrain_ds = load_tfrecord_dataset(\n    tfrecord_pattern.format(\"training\"), batch_size=1, shuffle=True\n)\nval_ds = load_tfrecord_dataset(\n    tfrecord_pattern.format(\"validation\"), batch_size=1, shuffle=False\n)\n</code></pre> <p>Model</p> <p>Build the <code>SwinUNETR</code> model with the specified input shape and number of classes.</p> <pre><code>num_classes=4\nmodel=SwinUNETR(\n    input_shape=(96, 96, 96, 1),\n    out_channels=num_classes,\n    classifier_activation=None,\n)\n\nmodel.compile(\n    optimizer=keras.optimizers.AdamW(\n        learning_rate=1e-4,\n        weight_decay=1e-5,\n    ),\n    loss=SparseDiceCELoss(from_logits=True),\n    metrics=[DiceMetric(\n        num_classes=num_classes,\n        include_background=True,\n        reduction=\"mean\",\n        ignore_empty=True,\n        smooth=1e-6,\n        name='dice_score'\n    )],\n    jit_compile=False,\n)\n</code></pre> <p>Sliding Window Inference Callback</p> <p>The Sliding Window Inference callback provides a convenient method for processing large volumetric samples efficiently. Instead of processing the entire volume at once (which may exceed memory limits), the input is divided into smaller overlapping windows. Each window is inferred separately, and the outputs are stitched together to form the final prediction. This approach helps in handling large 3D medical images while optimizing memory usage and ensuring accurate predictions.</p> <p><pre><code>swi_callback = SlidingWindowInferenceCallback(\n    model,\n    dataset=val_ds, \n    num_classes=num_classes,\n    overlap=0.8,\n    roi_size=(96, 96, 96),\n    sw_batch_size=4,\n    interval=100, \n    mode=\"constant\",\n    padding_mode=\"constant\",\n    sigma_scale=0.125,\n    cval=0.0,\n    roi_weight_map=0.8,\n    save_path=\"model.weights.h5\"\n)\n</code></pre> <pre><code>history = model.fit(\n    train_ds, \n    epochs=500,\n    callbacks=[\n        swi_callback\n    ]\n)\n</code></pre></p>"},{"location":"transformations/manage-transformations/","title":"Transformation","text":"<p>Transformation acts as a prprocessing and augmentation layers for model training. For 3D transformation, the expected format is: <code>depth, height, width, channel</code>. The transformation are implemented for a single sample. All the transformations are implemented using <code>tensorflow</code> in order to able to run with <code>tf.data</code> API with <code>keras</code> multi-backend library.</p> <pre><code>img_path = 'images/coronacases_001.nii.gz'\nmask_path = 'masks/coronacases_001.nii.gz'\n\nnib_x = nib.load(img_path) # (512, 512, 301)\nnib_y = nib.load(mask_path) # (512, 512, 301)\n\nimage = nib_x.get_fdata().transpose(2, 0, 1)[...,None] # (301, 512, 512, 1)\nlabel = nib_y.get_fdata().transpose(2, 0, 1)[...,None] # (301, 512, 512, 1)\n</code></pre>"},{"location":"transformations/manage-transformations/#preprocessing","title":"Preprocessing","text":"<p>Resize</p> <p>Resize the input image to given spatial size. Implemented using <code>tf.image.resize</code> and <code>depth_interpolate</code>.</p> <pre><code>from medicai.transforms import (\n    TensorBundle,\n    Resize,\n)\n\ninputs = TensorBundle({\"image\": image, \"label\": label})\ntransform = Resize(\n    keys=[\"image\", \"label\"], \n    spatial_shape=(96, 96, 96)\n)\n\noutput = transform(inputs)\ntransform_image = output.data[\"image\"]\ntransform_label = output.data[\"label\"]\ntransform_image.shape, transform_label.shape\n(TensorShape([96, 96, 96, 1]), TensorShape([96, 96, 96, 1]))\n</code></pre> <p>ScaleIntensityRange</p> <p>Scale the intensity of the entire array from the range <code>[a_min, a_max]</code> to <code>[b_min, b_max]</code>, with an optional clipping feature.</p> <pre><code>from medicai.transforms import (\n    TensorBundle,\n    ScaleIntensityRange,\n)\n\ninputs = TensorBundle({\"image\": image, \"label\": label})\ntransform = ScaleIntensityRange(\n    keys=[\"image\"], \n    a_min=-175,\n    a_max=250,\n    b_min=0.0,\n    b_max=1.0,\n    clip=True\n)\n\noutput = transform(inputs)\ntransform_image = output.data[\"image\"]\ntransform_label = output.data[\"label\"]\ntransform_image.shape, transform_label.shape\n\ntransform_image.numpy().min(), transform_image.numpy().max()\n(0.0, 1.0)\n\nnp.unique(transform_label)\narray([0., 1., 2., 3.])\n</code></pre> <p>CropForeground</p> <p>Crop an image using a bounding box, where the bounding box is generated by selecting the foreground through the <code>select_fn</code> function at the specified channel_indices. A margin is added to each spatial dimension of the bounding box.</p> <pre><code>from medicai.transforms import (\n    CropForeground,\n    TensorBundle,\n)\n\ninputs = TensorBundle({\"image\": image, \"label\": label})\ntransform = CropForeground(\n    keys=(\"image\", \"label\"), \n    source_key=\"image\"\n)\noutput = transform(inputs)\ntransform_image = output.data[\"image\"]\ntransform_label = output.data[\"label\"]\ntransform_image.shape, transform_label.shape\n((301, 512, 415, 1), (301, 512, 415, 1))\n</code></pre> <p>Spacing</p> <p>Resample input image into the specified pixdim. It will require <code>affine</code> meta information.</p> <pre><code>from medicai.transforms import (\n    TensorBundle,\n    Spacing,\n)\n\naffine = nib_x.affine\naffine[:, [0, 1, 2]] = affine[:, [2, 0, 1]]  # (H, W, D) -&gt; (D, H, W)\ntrans_affine = affine.astype(np.float32)\n\ninputs = TensorBundle(\n    {\n        \"image\": image, \n        \"label\": label\n    }, \n    meta={\n        'affine': trans_affine\n    }\n)\ntransform = Spacing(\n    keys=[\"image\", \"label\"], \n    pixdim=[2.0, 1.5, 1.5]\n)\n\noutput = transform(inputs)\ntransform_image = output.data[\"image\"]\ntransform_label = output.data[\"label\"]\ntransform_image.shape, transform_label.shape\n(TensorShape([121, 276, 341, 1]), TensorShape([121, 276, 341, 1]))\n</code></pre> <p>Orientation</p> <p>Change the orientation of the input image to the specified one based on the provided <code>axcodes</code>. It will require <code>affine</code> meta information.</p> <pre><code>from medicai.transforms import (\n    TensorBundle,\n    Orientation,\n)\n\naffine = nib_x.affine\naffine[:, [0, 1, 2]] = affine[:, [2, 0, 1]]  # (H, W, D) -&gt; (D, H, W)\ntrans_affine = affine.astype(np.float32)\n\ninputs = TensorBundle(\n    {\n        \"image\": image, \n        \"label\": label\n    }, \n    meta={\n        'affine': trans_affine\n    }\n)\ntransform = Orientation(\n    keys=[\"image\", \"label\"], \n    axcodes=\"RAS\"\n)\n\noutput = transform(inputs)\ntransform_image = output.data[\"image\"]\ntransform_label = output.data[\"label\"]\ntransform_image.shape, transform_label.shape\n(TensorShape([301, 512, 512, 1]), TensorShape([301, 512, 512, 1]))\n</code></pre>"},{"location":"transformations/manage-transformations/#radnom-preprocessing","title":"Radnom Preprocessing","text":"<p>RandRotate90</p> <p>Rotate the input sample 90 degree at random.</p> <pre><code>from medicai.transforms import (\n    TensorBundle,\n    RandRotate90,\n)\n\ninputs = TensorBundle({\"image\": image, \"label\": label})\ntransform = RandRotate90(\n    keys=[\"image\", \"label\"], \n    prob=1.0, \n    max_k=3\n)\noutput = transform(inputs)\n\ntransform_image = output.data[\"image\"]\ntransform_label = output.data[\"label\"]\ntransform_image.shape, transform_label.shape\n(TensorShape([301, 512, 512, 1]), TensorShape([301, 512, 512, 1]))\n</code></pre> <p>RandShiftIntensity</p> <p>Randomly shift the intensity of the image by applying a randomly selected offset.</p> <pre><code>from medicai.transforms import (\n    TensorBundle,\n    RandShiftIntensity,\n)\n\ninputs = TensorBundle({\"image\": image, \"label\": label})\ntransform = RandShiftIntensity(\n    keys=[\"image\", \"label\"], \n    offsets=(-0.2, 0.8), \n    prob=1.0\n)\n\noutput = transform(inputs)\ntransform_image = output.data[\"image\"]\ntransform_label = output.data[\"label\"]\ntransform_image.shape, transform_label.shape\n(TensorShape([301, 512, 512, 1]), TensorShape([301, 512, 512, 1]))\n</code></pre> <p>RandCropByPosNegLabel</p> <p>Randomly crop fixed-sized regions from the image, with the center of each crop being either a foreground or background voxel based on the specified Positive-Negative Ratio. The function will return a list of arrays for all the cropped images.</p> <pre><code>from medicai.transforms import (\n    TensorBundle,\n    RandCropByPosNegLabel,\n)\n\ninputs = TensorBundle({\"image\": image, \"label\": label})\ntransform = RandCropByPosNegLabel(\n    keys=[\"image\", \"label\"], \n    spatial_size=(96, 96, 96), \n    pos=1, \n    neg=1, \n    num_samples=1\n)\noutput = transform(inputs)\ntransform_image = output.data[\"image\"]\ntransform_label = output.data[\"label\"]\ntransform_image.shape, transform_label.shape\n(TensorShape([96, 96, 96, 1]), TensorShape([96, 96, 96, 1]))\n</code></pre>"},{"location":"transformations/manage-transformations/#compose","title":"Compose","text":"<p>The <code>Compose</code> api allow to chain a series of callables in a sequential order. Each transform in the sequence must accept a single argument and return a single value, enabling the creation of a pipeline of transformations that are applied one after another. A sample example is shown bleow.</p> <pre><code>from medicai.transforms import (\n    Compose,\n    Orientation,\n    Spacing,\n)\n\ntransform = Compose(\n    [\n        Orientation(keys=[\"image\", \"label\"], axcodes=\"RAS\"), \n        Spacing(keys=[\"image\", \"label\"], pixdim=[1.0, 1.2, 1.2])\n    ]\n)\ninputs = {\"image\": image, \"label\": label}\nmeta = {'affine': trans_affine}\noutput = transform(inputs, meta)\n\ntransform_image = output.data[\"image\"]\ntransform_label = output.data[\"label\"]\ntransform_image.shape, transform_label.shape\n(TensorShape([243, 345, 426, 1]), TensorShape([243, 345, 426, 1]))\n</code></pre>"}]}