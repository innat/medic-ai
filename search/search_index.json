{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#medicai","title":"MedicAI","text":"<p>The <code>medicai</code> is a Keras based library designed for medical image analysis using machine learning techniques. Its core strengths include:</p> <ul> <li>Backend Agnostic: Compatible with <code>tensorflow</code>, <code>torch</code>, and <code>jax</code>.</li> <li>User-Friendly API: High-level interface for transformations and model creation.</li> <li>Scalable Execution: Supports training and inference on single/multi-GPU and TPU-VM setups.</li> <li>Essential Components: Includes standard metrics and losses, such as Dice.</li> <li>Optimized 3D Inference: Offers an efficient sliding-window method and callback for volumetric data</li> </ul>"},{"location":"#installation","title":"\ud83d\udee0 Installation","text":"<p>PyPI version:</p> <pre><code>pip install medicai\n</code></pre> <p>Installing from source GitHub:</p> <pre><code>pip install git+https://github.com/innat/medic-ai.git\n</code></pre>"},{"location":"#available-features","title":"\ud83c\udf41 Available Features","text":"<p>The <code>medicai</code> library provides a range of features for medical image processing, model training, and inference. Below is an overview of its key functionalities.</p> <p>Image Transformations</p> <p><code>medicai</code> includes various transformation utilities for preprocessing medical images:</p> <ul> <li>Basic Transformations:<ul> <li><code>Resize</code>: Adjusts the image dimensions.</li> <li><code>ScaleIntensityRange</code>: Normalizes intensity values within a specified range.</li> <li><code>CropForeground</code>: Crops the image to focus on the region of interest.</li> <li><code>Spacing</code>: Resamples the image to a target voxel spacing.</li> <li><code>Orientation</code>: Standardizes image orientation.</li> <li><code>NormalizeIntensity</code>: Normalize the intensity of tensors based on global or channel-wise statistics.</li> <li><code>SignalFillEmpty</code>: Fills <code>nan</code>, positive infinity, and negative infinity values in specified tensors with a given replacement.</li> </ul> </li> <li>Augmentations for Robustness:<ul> <li><code>RandCropByPosNegLabel</code>: Randomly crops based on positive and negative label ratios.</li> <li><code>RandRotate90</code>: Randomly rotates images by 90 degrees.</li> <li><code>RandShiftIntensity</code>: Randomly shifts intensity values.</li> <li><code>RandFlip</code>: Randomly flips images along specified axes.</li> <li><code>RandomSpatialCrop</code>: Randomly crops a region of interest (ROI).</li> </ul> </li> <li>Pipeline Composition:<ul> <li><code>Compose</code>: Chains multiple transformations into a single pipeline.</li> </ul> </li> </ul> <p>Models</p> <p>Currently, <code>medicai</code> focuses on 3D models for classification and segmentation:</p> <ul> <li><code>SwinTransformer</code> \u2013 3D classification task.</li> <li><code>SwinUNETR</code> \u2013 3D segmentation task.</li> </ul> <p>Inference</p> <ul> <li><code>SlidingWindowInference</code> \u2013 Processes large 3D images in smaller overlapping windows, improving performance and memory efficiency.</li> </ul>"},{"location":"#guides","title":"\ud83d\udca1 Guides","text":"<p>Segmentation: Available guides for 3D segmentation task.</p> Task GitHub Kaggle Covid-19 BTCV BraTS Spleen <p>Classification: Available guides for 3D classification task.</p> Task (Classification) GitHub Kaggle Covid-19"},{"location":"#contributing","title":"\ud83e\udd1d Contributing","text":"<p>Please refer to the current roadmap for an overview of the project. Feel free to explore anything that interests you. If you have suggestions or ideas, I\u2019d appreciate it if you could open a GitHub issue so we can discuss them further.</p> <ol> <li>Install <code>medicai</code> from soruce:</li> </ol> <pre><code>!git clone https://github.com/innat/medic-ai\n%cd medic-ai\n!pip install keras -qU\n!pip install -e .\n%cd ..\n</code></pre> <p>Add your contribution and implement relevant test code.</p> <ol> <li>Run test code as:</li> </ol> <pre><code>python -m pytest test/\n\n# or, only one your new_method\npython -m pytest -k new_method\n</code></pre>"},{"location":"#acknowledgements","title":"\ud83d\ude4f Acknowledgements","text":"<p>This project is greatly inspired by MONAI.</p>"},{"location":"#citation","title":"\ud83d\udcdd Citation","text":"<p>If you use <code>medicai</code> in your research or educational purposes, please cite it using the metadata from our <code>CITATION.cff</code> file.</p>"},{"location":"general/","title":"General","text":""},{"location":"general/#generate-3d-nii-to-tfrecord-dataset","title":"Generate 3D <code>.nii</code> to TFRecord Dataset","text":"<p>The process is shown here.</p>"},{"location":"models/manage-models/","title":"Model","text":"<p>The <code>medicai</code> provides Swin Transformer and SwinUNETR models for 3D classification and segmentation respectively. These models are translated from official release to keras, and able to run on multiple backend, i.e., <code>tensorflow</code>, <code>torch</code>, and, <code>jax</code> backends.</p>"},{"location":"models/manage-models/#3d-models","title":"3D Models","text":"<p>Classification</p> <pre><code>import tensorflow as tf\nfrom medicai.models import SwinTransformer\n\nnum_classes = 4\ninput_shape = (96, 96, 96, 1)\nmodel = SwinTransformer(\n    input_shape=input_shape, \n    num_classes=num_classes, \n    classifier_activation=None\n)\n\ndummy_input = tf.random.normal((1, 96, 96, 96, 1))\noutput = model(dummy_input)\noutput.shape\nTensorShape([1, 4])\n</code></pre> <p>Segmentation</p> <pre><code>import tensorflow as tf\nfrom medicai.models import SwinUNETR\n\nnum_classes = 4\ninput_shape = (96, 96, 96, 1)\nmodel = SwinUNETR(\n    input_shape=input_shape, \n    num_classes=num_classes,\n    classifier_activation=None\n)\n\ndummy_input = tf.random.normal((1, 96, 96, 96, 1))\noutput = model(dummy_input)\noutput.shape\nTensorShape([1, 96, 96, 96, 4])\n</code></pre>"},{"location":"training/manage-training/","title":"Training","text":"<p>The following guides provide comprehensive, end-to-end examples covering data loading, model training, and evaluation workflows. You can use various types of data loaders, including <code>tf.data.Dataset</code>, <code>keras.utils.PyDataset</code>, <code>torch.utils.data.DataLoader</code>, or even a custom Python generator function. These workflows are designed to be flexible and can run seamlessly on a single device or scale across multiple GPUs or TPUs, depending on your setup.</p> <p>Segmentation: Available guides for 3D segmentation task.</p> Task GitHub Kaggle Covid-19 BTCV BraTS Spleen <p>Classification: Available guides for 3D classification task.</p> Task (Classification) GitHub Kaggle Covid-19"},{"location":"transformations/manage-transformations/","title":"Transformation","text":"<p>Transformation acts as a prprocessing and augmentation layers for model training. In <code>medicai</code> for 3D transformation, the expected format is: <code>depth, height, width, channel</code>. The transformation are implemented for a single sample. All the transformations are implemented using <code>tensorflow</code> in order to able to run with <code>tf.data</code> API with <code>keras</code> multi-backend library.</p> <pre><code>img_path = 'images/coronacases_001.nii.gz'\nmask_path = 'masks/coronacases_001.nii.gz'\n\nnib_x = nib.load(img_path) # (512, 512, 301)\nnib_y = nib.load(mask_path) # (512, 512, 301)\n\nimage = nib_x.get_fdata().transpose(2, 0, 1)[...,None] # (301, 512, 512, 1)\nlabel = nib_y.get_fdata().transpose(2, 0, 1)[...,None] # (301, 512, 512, 1)\n</code></pre>"},{"location":"transformations/manage-transformations/#preprocessing","title":"Preprocessing","text":"<p>Resize</p> <p>Resize the input image to given spatial size. Implemented using <code>tf.image.resize</code> and <code>depth_interpolate</code>.</p> <pre><code>from medicai.transforms import (\n    TensorBundle,\n    Resize,\n)\n\ninputs = TensorBundle({\"image\": image, \"label\": label})\ntransform = Resize(\n    keys=[\"image\", \"label\"], \n    spatial_shape=(96, 96, 96)\n)\n\noutput = transform(inputs)\ntransform_image = output.data[\"image\"]\ntransform_label = output.data[\"label\"]\ntransform_image.shape, transform_label.shape\n(TensorShape([96, 96, 96, 1]), TensorShape([96, 96, 96, 1]))\n</code></pre> <p>ScaleIntensityRange</p> <p>Scale the intensity of the entire array from the range <code>[a_min, a_max]</code> to <code>[b_min, b_max]</code>, with an optional clipping feature.</p> <pre><code>from medicai.transforms import (\n    TensorBundle,\n    ScaleIntensityRange,\n)\n\ninputs = TensorBundle({\"image\": image, \"label\": label})\ntransform = ScaleIntensityRange(\n    keys=[\"image\"], \n    a_min=-175,\n    a_max=250,\n    b_min=0.0,\n    b_max=1.0,\n    clip=True\n)\n\noutput = transform(inputs)\ntransform_image = output.data[\"image\"]\ntransform_label = output.data[\"label\"]\ntransform_image.shape, transform_label.shape\n\ntransform_image.numpy().min(), transform_image.numpy().max()\n(0.0, 1.0)\n\nnp.unique(transform_label)\narray([0., 1., 2., 3.])\n</code></pre> <p>CropForeground</p> <p>Crop an image using a bounding box, where the bounding box is generated by selecting the foreground through the <code>select_fn</code> function at the specified channel_indices. A margin is added to each spatial dimension of the bounding box.</p> <pre><code>from medicai.transforms import (\n    CropForeground,\n    TensorBundle,\n)\n\ninputs = TensorBundle({\"image\": image, \"label\": label})\ntransform = CropForeground(\n    keys=(\"image\", \"label\"), \n    source_key=\"image\"\n)\noutput = transform(inputs)\ntransform_image = output.data[\"image\"]\ntransform_label = output.data[\"label\"]\ntransform_image.shape, transform_label.shape\n((301, 512, 415, 1), (301, 512, 415, 1))\n</code></pre> <p>Spacing</p> <p>Resample input image into the specified pixdim. It will require <code>affine</code> meta information.</p> <pre><code>from medicai.transforms import (\n    TensorBundle,\n    Spacing,\n)\n\naffine = nib_x.affine\naffine[:, [0, 1, 2]] = affine[:, [2, 0, 1]]  # (H, W, D) -&gt; (D, H, W)\ntrans_affine = affine.astype(np.float32)\n\ninputs = TensorBundle(\n    {\n        \"image\": image, \n        \"label\": label\n    }, \n    meta={\n        'affine': trans_affine\n    }\n)\ntransform = Spacing(\n    keys=[\"image\", \"label\"], \n    pixdim=[2.0, 1.5, 1.5]\n)\n\noutput = transform(inputs)\ntransform_image = output.data[\"image\"]\ntransform_label = output.data[\"label\"]\ntransform_image.shape, transform_label.shape\n(TensorShape([121, 276, 341, 1]), TensorShape([121, 276, 341, 1]))\n</code></pre> <p>Orientation</p> <p>Change the orientation of the input image to the specified one based on the provided <code>axcodes</code>. It will require <code>affine</code> meta information.</p> <pre><code>from medicai.transforms import (\n    TensorBundle,\n    Orientation,\n)\n\naffine = nib_x.affine\naffine[:, [0, 1, 2]] = affine[:, [2, 0, 1]]  # (H, W, D) -&gt; (D, H, W)\ntrans_affine = affine.astype(np.float32)\n\ninputs = TensorBundle(\n    {\n        \"image\": image, \n        \"label\": label\n    }, \n    meta={\n        'affine': trans_affine\n    }\n)\ntransform = Orientation(\n    keys=[\"image\", \"label\"], \n    axcodes=\"RAS\"\n)\n\noutput = transform(inputs)\ntransform_image = output.data[\"image\"]\ntransform_label = output.data[\"label\"]\ntransform_image.shape, transform_label.shape\n(TensorShape([301, 512, 512, 1]), TensorShape([301, 512, 512, 1]))\n</code></pre>"},{"location":"transformations/manage-transformations/#radnom-preprocessing","title":"Radnom Preprocessing","text":"<p>RandRotate90</p> <p>Rotate the input sample 90 degree at random.</p> <pre><code>from medicai.transforms import (\n    TensorBundle,\n    RandRotate90,\n)\n\ninputs = TensorBundle({\"image\": image, \"label\": label})\ntransform = RandRotate90(\n    keys=[\"image\", \"label\"], \n    prob=1.0, \n    max_k=3\n)\noutput = transform(inputs)\n\ntransform_image = output.data[\"image\"]\ntransform_label = output.data[\"label\"]\ntransform_image.shape, transform_label.shape\n(TensorShape([301, 512, 512, 1]), TensorShape([301, 512, 512, 1]))\n</code></pre> <p>RandShiftIntensity</p> <p>Randomly shift the intensity of the image by applying a randomly selected offset.</p> <pre><code>from medicai.transforms import (\n    TensorBundle,\n    RandShiftIntensity,\n)\n\ninputs = TensorBundle({\"image\": image, \"label\": label})\ntransform = RandShiftIntensity(\n    keys=[\"image\", \"label\"], \n    offsets=(-0.2, 0.8), \n    prob=1.0\n)\n\noutput = transform(inputs)\ntransform_image = output.data[\"image\"]\ntransform_label = output.data[\"label\"]\ntransform_image.shape, transform_label.shape\n(TensorShape([301, 512, 512, 1]), TensorShape([301, 512, 512, 1]))\n</code></pre> <p>RandCropByPosNegLabel</p> <p>Randomly crop fixed-sized regions from the image, with the center of each crop being either a foreground or background voxel based on the specified Positive-Negative Ratio. The function will return a list of arrays for all the cropped images.</p> <pre><code>from medicai.transforms import (\n    TensorBundle,\n    RandCropByPosNegLabel,\n)\n\ninputs = TensorBundle({\"image\": image, \"label\": label})\ntransform = RandCropByPosNegLabel(\n    keys=[\"image\", \"label\"], \n    spatial_size=(96, 96, 96), \n    pos=1, \n    neg=1, \n    num_samples=1\n)\noutput = transform(inputs)\ntransform_image = output.data[\"image\"]\ntransform_label = output.data[\"label\"]\ntransform_image.shape, transform_label.shape\n(TensorShape([96, 96, 96, 1]), TensorShape([96, 96, 96, 1]))\n</code></pre>"},{"location":"transformations/manage-transformations/#compose","title":"Compose","text":"<p>The <code>Compose</code> api allow to chain a series of callables in a sequential order. Each transform in the sequence must accept a single argument and return a single value, enabling the creation of a pipeline of transformations that are applied one after another. A sample example is shown bleow.</p> <pre><code>from medicai.transforms import (\n    Compose,\n    Orientation,\n    Spacing,\n)\n\ntransform = Compose(\n    [\n        Orientation(keys=[\"image\", \"label\"], axcodes=\"RAS\"), \n        Spacing(keys=[\"image\", \"label\"], pixdim=[1.0, 1.2, 1.2])\n    ]\n)\ninputs = {\"image\": image, \"label\": label}\nmeta = {'affine': trans_affine}\noutput = transform(inputs, meta)\n\ntransform_image = output.data[\"image\"]\ntransform_label = output.data[\"label\"]\ntransform_image.shape, transform_label.shape\n(TensorShape([243, 345, 426, 1]), TensorShape([243, 345, 426, 1]))\n</code></pre>"}]}