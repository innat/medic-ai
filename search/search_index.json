{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#medicai","title":"MedicAI","text":"<p>Medic-AI is a Keras based library designed for medical image analysis using machine learning techniques. Its core strengths include:</p> <ul> <li>Backend Agnostic: Compatible with <code>tensorflow</code>, <code>torch</code>, and <code>jax</code> backends.</li> <li>User-Friendly API: High-level interface for transformations and model creation.</li> <li>Scalable Execution: Supports training and inference on single/multi-GPU and TPU-VM setups.</li> <li>Essential Components: Includes standard metrics and losses, such as Dice.</li> <li>Optimized 3D Inference: Offers an efficient sliding-window method and callback for volumetric data</li> </ul>"},{"location":"#table-of-contents","title":"\ud83d\udccb Table of Contents","text":"<ol> <li>Installation</li> <li>Features</li> <li>Guides</li> <li>Documentation</li> <li>Acknowledgements</li> <li>Citation</li> </ol>"},{"location":"#installation","title":"\ud83d\udee0 Installation","text":"<p>PyPI version:</p> <pre><code>!pip install medicai\n</code></pre> <p>Installing from source GitHub:</p> <pre><code>!pip install git+https://github.com/innat/medic-ai.git\n</code></pre>"},{"location":"#features","title":"\ud83d\udcca Features","text":"<p>Available Models : The following table lists the currently supported models along with their supported input modalities, primary tasks, and underlying architecture type.  The model inputs can be either 3D <code>(depth \u00d7 height \u00d7 width \u00d7 channel)</code> or 2D <code>(height \u00d7 width \u00d7 channel)</code>.</p> Model Supported Modalities Primary Task Architecture Type DenseNet121 2D, 3D Classification CNN DenseNet169 2D, 3D Classification CNN DenseNet201 2D, 3D Classification CNN ViT 2D, 3D Classification Transformer Swin Transformer 2D, 3D Classification Transformer DenseUNet121 2D, 3D Segmentation CNN DenseUNet169 2D, 3D Segmentation CNN DenseUNet201 2D, 3D Segmentation CNN UNETR 2D, 3D Segmentation Transformer SwinUNETR 2D, 3D Segmentation Transformer TransUNet 2D, 3D Segmentation Transformer SegFormer 2D, 3D Segmentation Transformer <p>Available Transformation: The following preprocessing and transformation methods are supported for volumetric data. The following layers are implemented with TensorFlow operations. It can be used in the <code>tf.data</code> API or a Python data generator and is fully compatible with multiple backends, <code>tf</code>, <code>torch</code>, <code>jax</code> in training and inference, supporting both GPUs and TPUs.</p> <pre><code>CropForeground\nNormalizeIntensity\nOrientation\nRandCropByPosNegLabel\nRandFlip\nRandRotate90\nRandShiftIntensity\nRandSpatialCrop\nResize\nScaleIntensityRange\nSpacing\n</code></pre>"},{"location":"#guides","title":"\ud83d\udca1 Guides","text":"<p>Segmentation: Available guides for 3D segmentation task.</p> Task GitHub Kaggle View Covid-19 BTCV n/a BraTS n/a Spleen <p>Classification: Available guides for 3D classification task.</p> Task (Classification) GitHub Kaggle Covid-19"},{"location":"#documentation","title":"\ud83d\udcda Documentation","text":"<p>To learn more about model, transformation, and training, please visit official documentation: <code>medicai/docs</code></p>"},{"location":"#contributing","title":"\ud83e\udd1d Contributing","text":"<p>Please refer to the current roadmap for an overview of the project. Feel free to explore anything that interests you. If you have suggestions or ideas, I\u2019d appreciate it if you could open a GitHub issue so we can discuss them further.</p> <ol> <li>Install <code>medicai</code> from soruce:</li> </ol> <pre><code>!git clone https://github.com/innat/medic-ai\n%cd medic-ai\n!pip install keras -qU\n!pip install -e .\n%cd ..\n</code></pre> <p>Add your contribution and implement relevant test code.</p> <ol> <li>Run test code as:</li> </ol> <pre><code>python -m pytest test/\n\n# or, only one your new_method\npython -m pytest -k new_method\n</code></pre>"},{"location":"#acknowledgements","title":"\ud83d\ude4f Acknowledgements","text":"<p>This project is greatly inspired by MONAI.</p>"},{"location":"#citation","title":"\ud83d\udcdd Citation","text":"<p>If you use <code>medicai</code> in your research or educational purposes, please cite it using the metadata from our <code>CITATION.cff</code> file.</p>"},{"location":"general/","title":"General","text":""},{"location":"general/#generate-3d-nii-to-tfrecord-dataset","title":"Generate 3D <code>.nii</code> to TFRecord Dataset","text":"<ol> <li>Convert COVID-19 CT Segmentation (20 Cases) to TFRecord.</li> <li>Convert Multi-Organ BTCV Abdomen to TFRecord</li> <li>Convert Multi-Modal BraTS to TFRecord</li> </ol>"},{"location":"guides/manage-guides/","title":"Code Examples","text":"<p>The following guides provide comprehensive, end-to-end examples covering data loading, model training, and evaluation workflows. You can use various types of data loaders, including <code>tf.data.Dataset</code>, <code>keras.utils.PyDataset</code>, <code>torch.utils.data.DataLoader</code>, or even a custom Python generator function. These workflows are designed to be flexible and can run seamlessly on a single device or scale across multiple GPUs or TPUs, depending on your setup.</p> <p>Segmentation: Available guides for 3D segmentation task.</p> Task GitHub Kaggle Covid-19 BTCV BraTS Spleen <p>Classification: Available guides for 3D classification task.</p> Task (Classification) GitHub Kaggle Covid-19"},{"location":"models/manage-models/","title":"Model","text":"<p>The <code>medic-ai</code> library provides state-of-the-art models for 2D and 3D medical image classification and segmentation. It features models based on both Convolutional Neural Networks (CNNs) and Transformers, which have been translated from their official releases to work with Keras. This allows them to function seamlessly across various backends, including TensorFlow, PyTorch, and JAX. The model inputs can be either 3D <code>(depth \u00d7 height \u00d7 width \u00d7 channel)</code> or 2D <code>(height \u00d7 width \u00d7 channel)</code>. The following table lists the currently supported models along with their supported input modalities, primary tasks, and underlying architecture type.  </p> Model Supported Modalities Primary Task Architecture Type DenseNet121 2D, 3D Classification CNN DenseNet169 2D, 3D Classification CNN DenseNet201 2D, 3D Classification CNN ViT 2D, 3D Classification Transformer Swin Transformer 2D, 3D Classification Transformer DenseUNet121 2D, 3D Segmentation CNN DenseUNet169 2D, 3D Segmentation CNN DenseUNet201 2D, 3D Segmentation CNN UNETR 2D, 3D Segmentation Transformer SwinUNETR 2D, 3D Segmentation Transformer TransUNet 2D, 3D Segmentation Transformer SegFormer 2D, 3D Segmentation Transformer <p>All models in <code>medicai</code> are flexible and can be built as either 2D or 3D models. The library automatically configures the model based on the provided <code>input_shape</code> argument. Specifying <code>(depth, height, width, channel)</code> creates a 3D model, whereas passing <code>(height, width, channel)</code> builds a 2D model.</p>"},{"location":"models/manage-models/#densenet121","title":"DenseNet121","text":"<p>A 2D or 3D DenseNet-121 model for classification task.</p> <pre><code>medicai.models.DenseNet121(\n    input_shape,\n    include_rescaling=False,\n    include_top=True,\n    num_classes=1000,\n    pooling=None,\n    classifier_activation=\"softmax\",\n    name=None,\n)\n\n# Build 3D model.\nnum_classes = 1\ninput_shape = (64, 64, 64, 1)\nmodel = medicai.models.DenseNet121(\n    input_shape=input_shape, num_classes=num_classes\n)\n\n# Build 2D model.\ninput_shape = (64, 64, 1)\nmodel = medicai.models.DenseNet121(\n    input_shape=input_shape, \n    num_classes=num_classes\n)\n</code></pre> <p>Reference</p> <ul> <li>Densely Connected Convolutional Networks (CVPR 2017)</li> </ul> <p>Arguments</p> <ul> <li>include_top: whether to include the fully-connected layer at the top of the network.</li> <li>input_shape: Input tensor shape, excluding batch size. It can be either <code>(depth, height, width, channel)</code> or <code>(height, width, channel)</code>.</li> <li>include_rescaling: Whether to include input rescaling layer</li> <li>pooling: Optional pooling mode for feature extraction when <code>include_top</code> is <code>False</code>.<ul> <li><code>None</code> means that the output of the model will be the 4D/5D tensor output of the last convolutional layer.</li> <li><code>avg</code> means that global average pooling will be applied to the output of the last convolutional layer, and thus the output of the model will be a 2D/3D tensor.</li> <li><code>max</code> means that global max pooling will be applied.</li> </ul> </li> <li>num_classes: Number of classes to classify samples.</li> <li>classifier_activation: The activation function to use on the top layer.</li> <li>name: The name of the model.</li> </ul>"},{"location":"models/manage-models/#densenet169","title":"DenseNet169","text":"<p>A 2D or 3D DenseNet-169 model for classification task.</p> <pre><code>medicai.models.DenseNet169(\n    input_shape,\n    include_rescaling=False,\n    include_top=True,\n    num_classes=1000,\n    pooling=None,\n    classifier_activation=\"softmax\",\n    name=None,\n)\n\n# Build 3D model.\nnum_classes = 1\ninput_shape = (64, 64, 64, 1)\nmodel = medicai.models.DenseNet169(\n    input_shape=input_shape, num_classes=num_classes\n)\n\n# Build 2D model.\ninput_shape = (64, 64, 1)\nmodel = medicai.models.DenseNet169(\n    input_shape=input_shape, \n    num_classes=num_classes\n)\n</code></pre> <p>Reference</p> <ul> <li>Densely Connected Convolutional Networks (CVPR 2017)</li> </ul> <p>Arguments</p> <ul> <li>include_top: whether to include the fully-connected layer at the top of the network.</li> <li>input_shape: Input tensor shape, excluding batch size. It can be either <code>(depth, height, width, channel)</code> or <code>(height, width, channel)</code>.</li> <li>include_rescaling: Whether to include input rescaling layer</li> <li>pooling: Optional pooling mode for feature extraction when <code>include_top</code> is <code>False</code>.<ul> <li><code>None</code> means that the output of the model will be the 4D/5D tensor output of the last convolutional layer.</li> <li><code>avg</code> means that global average pooling will be applied to the output of the last convolutional layer, and thus the output of the model will be a 2D/3D tensor.</li> <li><code>max</code> means that global max pooling will be applied.</li> </ul> </li> <li>num_classes: Number of classes to classify samples.</li> <li>classifier_activation: The activation function to use on the top layer.</li> <li>name: The name of the model.</li> </ul>"},{"location":"models/manage-models/#densenet201","title":"DenseNet201","text":"<p>A 2D or 3D DenseNet-201 model for classification task.</p> <pre><code>medicai.models.DenseNet201(\n    input_shape,\n    include_rescaling=False,\n    include_top=True,\n    num_classes=1000,\n    pooling=None,\n    classifier_activation=\"softmax\",\n    name=None,\n)\n\n# Build 3D model.\nnum_classes = 1\ninput_shape = (64, 64, 64, 1)\nmodel = medicai.models.DenseNet201(\n    input_shape=input_shape, num_classes=num_classes\n)\n\n# Build 2D model.\ninput_shape = (64, 64, 1)\nmodel = medicai.models.DenseNet201(\n    input_shape=input_shape, \n    num_classes=num_classes\n)\n</code></pre> <p>Reference</p> <ul> <li>Densely Connected Convolutional Networks (CVPR 2017)</li> </ul> <p>Arguments</p> <ul> <li>include_top: whether to include the fully-connected layer at the top of the network.</li> <li>input_shape: Input tensor shape, excluding batch size. It can be either <code>(depth, height, width, channel)</code> or <code>(height, width, channel)</code>.</li> <li>include_rescaling: Whether to include input rescaling layer</li> <li>pooling: Optional pooling mode for feature extraction when <code>include_top</code> is <code>False</code>.<ul> <li><code>None</code> means that the output of the model will be the 4D/5D tensor output of the last convolutional layer.</li> <li><code>avg</code> means that global average pooling will be applied to the output of the last convolutional layer, and thus the output of the model will be a 2D/3D tensor.</li> <li><code>max</code> means that global max pooling will be applied.</li> </ul> </li> <li>num_classes: Number of classes to classify samples.</li> <li>classifier_activation: The activation function to use on the top layer.</li> <li>name: The name of the model.</li> </ul>"},{"location":"models/manage-models/#vision-transformer-vit","title":"Vision Transformer (ViT)","text":"<p>A 2D and 3D Vision Transformer (ViT) model for classification.</p> <p>This class implements a Vision Transformer (ViT) model, supporting both 2D and 3D inputs. The model consists of a ViT backbone, optional intermediate pre-logits layer, dropout, and a classification head</p> <pre><code>medicai.models.ViT(\n    input_shape,\n    num_classes,\n    patch_size=16,\n    num_layers=12,\n    num_heads=12,\n    hidden_dim=768,\n    mlp_dim=3072,\n    pooling=\"token\",\n    intermediate_dim=None,\n    classifier_activation=None,\n    dropout=0.0,\n    name=\"vit\",\n)\n\n# Build 3D model.\ninput_shape = (16, 32, 32, 1)\nnum_classes = 10\nmodel = medicai.models.ViT(\n    input_shape=input_shape, \n    num_classes=num_classes\n)\n\n# Build 2D model.\ninput_shape = (32, 32, 1)\nmodel = medicai.models.ViT(\n    input_shape=input_shape, \n    num_classes=num_classes\n)\n</code></pre> <p>Reference</p> <ul> <li>An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale (CVPR 2020)</li> </ul> <p>Arguments</p> <ul> <li>input_shape (tuple): Shape of the input tensor excluding batch size.     For example, <code>(height, width, channels)</code> for 2D     or <code>(depth, height, width, channels)</code> for 3D.</li> <li>num_classes (int): Number of output classes for classification.</li> <li>patch_size (int or tuple): Size of the patches extracted from the input.</li> <li>num_layers (int): Number of transformer encoder layers.</li> <li>num_heads (int): Number of attention heads in each transformer layer.</li> <li>hidden_dim (int): Hidden dimension size of the transformer encoder.</li> <li>mlp_dim (int): Hidden dimension size of the MLP in transformer blocks.</li> <li>pooling (str): Pooling strategy for the output. <code>token</code> for CLS token,     <code>gap</code> for global average pooling over spatial dimensions.</li> <li>intermediate_dim (int, optional): Dimension of optional pre-logits dense layer.     If <code>None</code>, no intermediate layer is used.</li> <li>classifier_activation (str, optional): Activation function for the output layer.</li> <li>dropout (float): Dropout rate applied before the output layer.</li> <li>name (str): Name of the model.</li> </ul>"},{"location":"models/manage-models/#swin-transformer","title":"Swin Transformer","text":"<p>A 2D and 3D Swin Transformer model for classification.</p> <p>This model utilizes the Swin Transformer backbone for feature extraction from 2D or 3D input data and includes a global average pooling layer followed by a dense layer for classification.</p> <pre><code>medicai.models.SwinTransformer(\n    input_shape,\n    num_classes,\n    classifier_activation=None,\n    name=\"swin_transformer\",\n)\n\n# 3D model.\nnum_classes = 4\ninput_shape = (96, 96, 96, 1)\nmodel = medicai.models.SwinTransformer(\n    input_shape=input_shape, \n    num_classes=num_classes\n)\n\n# 2D model.\nnum_classes = 4\ninput_shape = (96, 96, 1)\nmodel = medicai.models.SwinTransformer(\n    input_shape=input_shape, \n    num_classes=num_classes\n)\n</code></pre> <p>Reference</p> <ul> <li>Swin Transformer: Hierarchical Vision Transformer using Shifted Windows (CVPR 2021)</li> </ul> <p>Arguments</p> <ul> <li>input_shape (tuple): Shape of the input tensor excluding batch size.     For example, <code>(height, width, channels)</code> for 2D     or <code>(depth, height, width, channels)</code> for 3D.</li> <li>num_classes (int): Number of output classes for classification.</li> <li>classifier_activation (str, optional): Activation function for the output layer.</li> <li>name (str): Name of the model.</li> </ul>"},{"location":"models/manage-models/#denseunet121","title":"DenseUNet121","text":"<p>A UNet model with a DenseNet-121 backbone</p> <p>This model is a UNet architecture for image segmentation that uses a DenseNet-121 as its feature-extracting encoder. It's built to provide a powerful and flexible solution for both 2D and 3D segmentation tasks. .</p> <pre><code>medicai.models.DenseUNet121(\n    input_shape,\n    num_classes,\n    classifier_activation=None,\n    decoder_block_type=\"upsampling\",\n    decoder_filters=(256, 128, 64, 32, 16),\n    name='dense_unet_121',\n)\n\n# 3D model.\nnum_classes = 1\ninput_shape = (64, 64, 64, 1)\nmodel = medicai.models.DenseUNet121(\n    input_shape=input_shape,\n    num_classes=num_classes\n)\n\n# 2D model.\nnum_classes = 1\ninput_shape = (64, 64, 1)\nmodel = medicai.models.DenseUNet121(\n    input_shape=input_shape, \n    num_classes=num_classes\n)\n</code></pre> <p>Reference</p> <ul> <li>U-Net: Convolutional Networks for Biomedical Image Segmentation (CVPR 2015)</li> </ul> <p>Arguments</p> <ul> <li>input_shape (tuple): Shape of the input tensor excluding batch size.     For example, <code>(height, width, channels)</code> for 2D     or <code>(depth, height, width, channels)</code> for 3D.</li> <li>num_classes (int): Number of output classes for classification.</li> <li>classifier_activation (str, optional): Activation function for the output layer.</li> <li>decoder_block_type: Decoder block type, either <code>upsampling</code> or <code>transpose</code>.</li> <li>decoder_filters: The projection filters in decoder blocks. Default: <code>(256, 128, 64, 32, 16)</code>.</li> <li>name (str): Name of the model.</li> </ul>"},{"location":"models/manage-models/#denseunet169","title":"DenseUNet169","text":"<p>A UNet model with a DenseNet-169 backbone</p> <p>This model is a UNet architecture for image segmentation that uses a DenseNet-169 as its feature-extracting encoder. It's built to provide a powerful and flexible solution for both 2D and 3D segmentation tasks. .</p> <pre><code>medicai.models.DenseUNet169(\n    input_shape,\n    num_classes,\n    classifier_activation=None,\n    decoder_block_type=\"upsampling\",\n    decoder_filters=(256, 128, 64, 32, 16),\n    name='dense_unet_121',\n)\n\n# 3D model.\nnum_classes = 1\ninput_shape = (64, 64, 64, 1)\nmodel = medicai.models.DenseUNet169(\n    input_shape=input_shape,\n    num_classes=num_classes\n)\n\n# 2D model.\nnum_classes = 1\ninput_shape = (64, 64, 1)\nmodel = medicai.models.DenseUNet169(\n    input_shape=input_shape, \n    num_classes=num_classes\n)\n</code></pre> <p>Reference</p> <ul> <li>U-Net: Convolutional Networks for Biomedical Image Segmentation (CVPR 2015)</li> </ul> <p>Arguments</p> <ul> <li>input_shape (tuple): Shape of the input tensor excluding batch size.     For example, <code>(height, width, channels)</code> for 2D     or <code>(depth, height, width, channels)</code> for 3D.</li> <li>num_classes (int): Number of output classes for classification.</li> <li>classifier_activation (str, optional): Activation function for the output layer.</li> <li>decoder_block_type: Decoder block type, either <code>upsampling</code> or <code>transpose</code>.</li> <li>decoder_filters: The projection filters in decoder blocks. Default: <code>(256, 128, 64, 32, 16)</code>.</li> <li>name (str): Name of the model.</li> </ul>"},{"location":"models/manage-models/#denseunet201","title":"DenseUNet201","text":"<p>A UNet model with a DenseNet-201 backbone</p> <p>This model is a UNet architecture for image segmentation that uses a DenseNet-201 as its feature-extracting encoder. It's built to provide a powerful and flexible solution for both 2D and 3D segmentation tasks. .</p> <pre><code>medicai.models.DenseUNet201(\n    input_shape,\n    num_classes,\n    classifier_activation=None,\n    decoder_block_type=\"upsampling\",\n    decoder_filters=(256, 128, 64, 32, 16),\n    name='dense_unet_121',\n)\n\n# 3D model.\nnum_classes = 1\ninput_shape = (64, 64, 64, 1)\nmodel = medicai.models.DenseUNet201(\n    input_shape=input_shape,\n    num_classes=num_classes\n)\n\n# 2D model.\nnum_classes = 1\ninput_shape = (64, 64, 1)\nmodel = medicai.models.DenseUNet201(\n    input_shape=input_shape, \n    num_classes=num_classes\n)\n</code></pre> <p>Reference</p> <ul> <li>U-Net: Convolutional Networks for Biomedical Image Segmentation (CVPR 2015)</li> </ul> <p>Arguments</p> <ul> <li>input_shape (tuple): Shape of the input tensor excluding batch size.     For example, <code>(height, width, channels)</code> for 2D     or <code>(depth, height, width, channels)</code> for 3D.</li> <li>num_classes (int): Number of output classes for classification.</li> <li>classifier_activation (str, optional): Activation function for the output layer.</li> <li>decoder_block_type: Decoder block type, either <code>upsampling</code> or <code>transpose</code>.</li> <li>decoder_filters: The projection filters in decoder blocks. Default: <code>(256, 128, 64, 32, 16)</code>.</li> <li>name (str): Name of the model.</li> </ul>"},{"location":"models/manage-models/#unetr","title":"UNETR","text":"<p>UNETR: U-Net with a Vision Transformer (ViT) backbone for 3D and 2D medical image segmentation.</p> <p>UNETR integrates a ViT encoder as the backbone with a UNet-style decoder, using projection upsampling blocks and skip connections from intermediate transformer layers.</p> <pre><code>medicai.models.UNETR(\n    input_shape,\n    num_classes,\n    classifier_activation=None,\n    feature_size = 16,\n    hidden_size = 768,\n    mlp_dim = 3072,\n    num_heads = 12,\n    num_layers = 12,\n    patch_size = 16,\n    norm_name = \"instance\",\n    conv_block = True,\n    res_block = True,\n    dropout_rate = 0.0,\n    name = \"UNETR\",\n)\n\n# 3D model.\nnum_classes = 1\ninput_shape = (64, 64, 64, 1)\nmodel = medicai.models.UNETR(\n    input_shape=input_shape, \n    num_classes=num_classes\n)\n\n# 2D model.\nnum_classes = 1\ninput_shape = (64, 64, 1)\nmodel = medicai.models.UNETR(\n    input_shape=input_shape, \n    num_classes=num_classes\n)\n</code></pre> <p>Reference</p> <ul> <li>UNETR: Transformers for 3D Medical Image Segmentation (CVPR 2021)</li> </ul> <p>Arguments</p> <ul> <li>input_shape (tuple): Shape of the input tensor excluding batch size.     For example, <code>(height, width, channels)</code> for 2D     or <code>(depth, height, width, channels)</code> for 3D.</li> <li>num_classes (int): Number of output segmentation classes.</li> <li>classifier_activation (str, optional): Activation function applied to the output layer.</li> <li>feature_size (int): Base number of feature channels in decoder blocks.</li> <li>hidden_size (int): Hidden size of the transformer encoder.</li> <li>mlp_dim (int): Hidden size of MLPs in transformer blocks.</li> <li>num_heads (int): Number of attention heads per transformer layer.</li> <li>num_layers (int): Number of transformer encoder layers.</li> <li>patch_size (int): Size of the patches extracted from input.</li> <li>norm_name (str): Type of normalization for decoder blocks (<code>instance</code>, <code>batch</code>, etc.).</li> <li>conv_block (bool): Whether to use convolutional blocks in decoder.</li> <li>res_block (bool): Whether to use residual blocks in decoder.</li> <li>dropout_rate (float): Dropout rate applied in backbone and intermediate layers.</li> <li>name (str): Model name.</li> </ul>"},{"location":"models/manage-models/#swinunetr","title":"SwinUNETR","text":"<p>Swin-UNETR: A hybrid transformer-CNN for 3D or 2D medical image segmentation.</p> <p>This model combines the strengths of the Swin Transformer for feature extraction and a U-Net-like architecture for segmentation. It uses a Swin Transformer backbone to encode the input and a decoder with upsampling and skip connections to generate segmentation maps.</p> <pre><code>medicai.models.SwinUNETR(\n    input_shape,\n    num_classes,\n    classifier_activation=None,\n    feature_size=48,\n    norm_name=\"instance\",\n    res_block = True,\n    name = \"SwinUNETR\",\n)\n\n# 3D model.\nnum_classes = 4\ninput_shape = (96, 96, 96, 1)\nmodel = medicai.models.SwinUNETR(\n    input_shape=input_shape, \n    num_classes=num_classes,\n    classifier_activation=None\n)\n\n# 2D model.\ninput_shape = (96, 96, 1)\nmodel = medicai.models.SwinUNETR(\n    input_shape=input_shape, \n    num_classes=num_classes,\n    classifier_activation=None\n)\n</code></pre> <p>Reference</p> <ul> <li>Swin UNETR: Swin Transformers for Semantic Segmentation of Brain Tumors in MRI Images (CVPR 2022)</li> </ul> <p>Arguments</p> <ul> <li>input_shape (tuple): Shape of the input tensor excluding batch size.     For example, <code>(height, width, channels)</code> for 2D     or <code>(depth, height, width, channels)</code> for 3D.</li> <li>num_classes (int): Number of output segmentation classes.</li> <li>classifier_activation (str, optional): Activation function applied to the output layer.</li> <li>feature_size (int): The base feature map size in the decoder. Default is <code>48</code>.</li> <li>norm_name (str): Type of normalization for decoder blocks (<code>instance</code>, <code>batch</code>, etc.).</li> <li>res_block (bool): Whether to use residual blocks in decoder. Default is True.</li> <li>name (str): Model name.</li> </ul>"},{"location":"models/manage-models/#transunet","title":"TransUNet","text":"<p>TransUNet model for 2D or 3D semantic segmentation.</p> <p>This model combines a 3D or 2D CNN encoder (DenseNet) with a Vision Transformer (ViT) encoder and a hybrid decoder. The CNN extracts multi-scale local features, while the ViT captures global context. The decoder upsamples the fused features to produce the final segmentation map using a coarse-to-fine attention mechanism and U-Net-style skip connections.</p> <pre><code>medicai.models.TransUNet(\n    input_shape,\n    num_classes,\n    patch_size=3,\n    classifier_activation=None,\n    num_encoder_layers=6,\n    num_heads=8,\n    num_queries=100,\n    embed_dim=256,\n    mlp_dim=1024,\n    dropout_rate=0.1,\n    decoder_projection_filters=64,\n    name=None,\n)\n\n# 3D model.\nnum_classes = 4\npatch_size = 3\ninput_shape = (96, 96, 96, 1)\nmodel = medicai.models.TransUNet(\n    input_shape=input_shape, \n    num_classes=num_classes,\n    patch_size=patch_size,\n    classifier_activation=None\n)\n\n# 2D model.\ninput_shape = (96, 96, 1)\nmodel = medicai.models.TransUNet(\n    input_shape=input_shape, \n    num_classes=num_classes,\n    patch_size=patch_size,\n    classifier_activation=None\n)\n</code></pre> <p>Reference</p> <ul> <li>3D TransUNet: Advancing Medical Image Segmentation through Vision Transformers (CVPR 2023)</li> </ul> <p>Arguments</p> <ul> <li>input_shape (tuple): The shape of the input data. For 2D, it is     <code>(height, width, channels)</code>. For 3D, it is <code>(depth, height, width, channels)</code>.</li> <li>num_classes (int): The number of segmentation classes.</li> <li>patch_size (int or tuple): The size of the patches for the Vision     Transformer. Must be a tuple of length <code>spatial_dims</code>. Defaults to 3.</li> <li>num_queries (int, optional): The number of learnable queries used in the     decoder's attention mechanism. Defaults to <code>100</code>.</li> <li>classifier_activation (str, optional): Activation function for the final     segmentation head (e.g., <code>sigmoid</code> for binary, <code>softmax</code> for multi-class).</li> <li>num_encoder_layers (int, optional): The number of transformer encoder blocks     in the <code>ViT</code> encoder. Defaults to <code>6</code>.</li> <li>num_heads (int, optional): The number of attention heads in the transformer blocks.     Defaults to <code>8</code>.</li> <li>embed_dim (int, optional): The dimensionality of the token embeddings.     Defaults to <code>256</code>.</li> <li>mlp_dim (int, optional): The hidden dimension of the MLP in the transformer     blocks. Defaults to <code>1024</code>.</li> <li>dropout_rate (float, optional): The dropout rate for regularization.     Defaults to <code>0.1</code>.</li> <li>decoder_projection_filters (int, optional): The number of filters for the     convolutional layers in the decoder upsampling path. Defaults to <code>64</code>.</li> <li>name (str, optional): The name of the model. Defaults to <code>TransUNetND</code>.</li> </ul> <p>Note: The 3D-TransUNet model combines a CNN and a Transformer in its encoder and decoder. While the original version's encoder uses a ResNet-like CNN, the <code>medicai</code> implementation uses a Densenet-like feature extractor.</p>"},{"location":"models/manage-models/#segformer","title":"SegFormer","text":"<p>SegFormer model for 2D or 3D semantic segmentation.</p> <p>This class implements the full SegFormer architecture, which combines a hierarchical MixVisionTransformer (MiT) encoder with a lightweight MLP decoder head. This design is highly efficient for semantic segmentation tasks on high-resolution images or volumes.</p> <p>The encoder progressively downsamples the spatial dimensions and increases the feature dimensions across four stages, producing multi-scale feature maps. The decoder then takes these features, processes them through linear layers, upsamples them to a common resolution, and fuses them to generate a high-resolution segmentation mask.</p> <pre><code>medicai.models.SegFormer(\n    input_shape,\n    num_classes,\n    decoder_head_embedding_dim=256,\n    classifier_activation=None,\n    qkv_bias=True,\n    dropout=0.0,\n    project_dim=[32, 64, 160, 256],\n    layerwise_sr_ratios=[4, 2, 1, 1],\n    layerwise_patch_sizes=[7, 3, 3, 3],\n    layerwise_strides=[4, 2, 2, 2],\n    layerwise_num_heads=[1, 2, 5, 8],\n    layerwise_depths=[2, 2, 2, 2],\n    layerwise_mlp_ratios=[4, 4, 4, 4],\n    name=None,\n)\n\n# 3D model.\nnum_classes = 4\ninput_shape = (96, 96, 96, 1)\nmodel = medicai.models.SegFormer(\n    input_shape=input_shape, \n    num_classes=num_classes,\n)\n\n# 2D model.\ninput_shape = (96, 96, 1)\nmodel = medicai.models.SegFormer(\n    input_shape=input_shape, \n    num_classes=num_classes,\n)\n</code></pre> <p>Reference</p> <ul> <li>SegFormer3D: an Efficient Transformer for 3D Medical Image Segmentation (CVPR 2024)</li> </ul> <p>Arguments</p> <ul> <li>input_shape (tuple): The shape of the input data, excluding the batch dimension.</li> <li>num_classes (int): The number of output classes for segmentation.</li> <li>decoder_head_embedding_dim (int, optional): The embedding dimension of the decoder head.     Defaults to 256.</li> <li>classifier_activation (str, optional): The activation function for the final output     layer. Common choices are <code>softmax</code> for multi-class segmentation and <code>sigmoid</code> for multi-label or binary segmentation. Defaults to <code>None</code>.</li> <li>qkv_bias (bool, optional): Whether to include a bias in the query, key, and value       projections. Defaults to <code>True</code>.</li> <li>dropout (float, optional): The dropout rate for the decoder head. Defaults to 0.0.</li> <li>project_dim (list[int], optional): A list of feature dimensions for each encoder stage.     Defaults to <code>[32, 64, 160, 256]</code>.</li> <li>layerwise_sr_ratios (list[int], optional): A list of spatial reduction ratios for each     encoder stage's attention layers. Defaults to <code>[4, 2, 1, 1]</code>.</li> <li>layerwise_patch_sizes (list[int], optional): A list of patch sizes for the embedding     layer in each encoder stage. Defaults to <code>[7, 3, 3, 3]</code>.</li> <li>layerwise_strides (list[int], optional): A list of strides for the embedding layer in     each encoder stage. Defaults to <code>[4, 2, 2, 2]</code>.</li> <li>layerwise_num_heads (list[int], optional): A list of the number of attention heads for     each encoder stage. Defaults to <code>[1, 2, 5, 8]</code>.</li> <li>layerwise_depths (list[int], optional): A list of the number of transformer blocks for     each encoder stage. Defaults to <code>[2, 2, 2, 2]</code>.</li> <li>layerwise_mlp_ratios (list[int], optional): A list of MLP expansion ratios for each     encoder stage. Defaults to <code>[4, 4, 4, 4]</code>.</li> <li>name (str, optional): The name of the model. Defaults to <code>None</code>.</li> </ul>"},{"location":"transformations/manage-transformations/","title":"Transformation","text":"<p>Transformation acts as a prprocessing and augmentation layers for model training. In <code>medicai</code> for 3D transformation, the expected format is: <code>depth, height, width, channel</code>. The transformation are implemented for a single sample. All the transformations are implemented using <code>tensorflow</code> in order to able to run with <code>tf.data</code> API with <code>keras</code> multi-backend library.</p> <p>Run the following code in kaggle environment: 3D Transformation. It contains side by side comparison between <code>medicai</code> and <code>monai</code>.</p>"},{"location":"transformations/manage-transformations/#load-image","title":"Load Image","text":"<pre><code>def PyLoadImage(image_path, label_path):\n    # load data\n    image_nii = nib.load(image_path)\n    label_nii = nib.load(label_path)\n    image = image_nii.get_fdata().astype(np.float32)\n    label = label_nii.get_fdata().astype(np.float32)\n    affine = np.array(image_nii.affine, dtype=np.float32)\n\n    # re-arrange shape [whd -&gt; dhw]\n    image = np.transpose(image, (2, 1, 0))\n    label = np.transpose(label, (2, 1, 0))\n    affine[:, :3] = affine[:, [2, 1, 0]]\n\n    # add channel axis\n    image = image[..., np.newaxis] if image.ndim == 3 else image\n    label = label[..., np.newaxis] if label.ndim == 3 else label\n\n    # pack to dict\n    data = {}\n    meta = {}\n    data['image'] = image\n    data['label'] = label\n    meta['affine'] = affine\n    return data, meta\n\nimg_path = 'images/coronacases_001.nii.gz'\nmask_path = 'masks/coronacases_001.nii.gz'\n</code></pre>"},{"location":"transformations/manage-transformations/#resize","title":"Resize","text":"<p>Resize the input image to given spatial size. Implemented using <code>tf.image.resize</code> and <code>depth_interpolate</code>.</p> <pre><code>from medicai.transforms import (\n    Compose,\n    Resize,\n)\n\n# define transformations\ntransform = Compose([\n    Resize(\n        keys=[\"image\", \"label\"], \n        spatial_shape=(96, 96, 96),\n        mode=(\"bilinear\", \"nearest\")\n    )\n])\n\n# load the raw data\ndata, meta = PyLoadImage(image_path=image_path, label_path=mask_path)\n\n# passing sample to medicai transform\noutput = transform(data, meta)\n\n# get transformed sample\nmedicai_image = output['image'].numpy()\nmedicai_label = output['label'].numpy()\n</code></pre>"},{"location":"transformations/manage-transformations/#scaleintensity-range","title":"ScaleIntensity Range","text":"<p>Scale the intensity of the entire array from the range <code>[a_min, a_max]</code> to <code>[b_min, b_max]</code>, with an optional clipping feature.</p> <pre><code>from medicai.transforms import (\n    Compose,\n    ScaleIntensityRange,\n)\n\n# define transformations\ntransform = Compose([\n    ScaleIntensityRange(\n        keys=[\"image\"],\n        a_min=-175,\n        a_max=250,\n        b_min=0.0,\n        b_max=1.0,\n        clip=True,\n    )\n])\n\n# load the raw data\ndata, meta = PyLoadImage(image_path=image_path, label_path=mask_path)\n\n# passing sample to medicai transform\noutput = transform(data, meta)\n\n# get transformed sample\nmedicai_image = output['image'].numpy()\nmedicai_label = output['label'].numpy()\n</code></pre>"},{"location":"transformations/manage-transformations/#crop-foreground","title":"Crop Foreground","text":"<p>Crop an image using a bounding box, where the bounding box is generated by selecting the foreground through the <code>select_fn</code> function at the specified channel_indices. A margin is added to each spatial dimension of the bounding box.</p> <pre><code>from medicai.transforms import (\n    Compose,\n    CropForeground,\n)\n\n# define transformations\ntransform = Compose([\n    CropForeground(\n        keys=(\"image\", \"label\"), \n        source_key=\"image\"\n    )\n])\n\n# load the raw data\ndata, meta = PyLoadImage(image_path=image_path, label_path=mask_path)\n\n# passing sample to medicai transform\noutput = transform(data, meta)\n\n# get transformed sample\nmedicai_image = output['image'].numpy()\nmedicai_label = output['label'].numpy()\n</code></pre>"},{"location":"transformations/manage-transformations/#spacing","title":"Spacing","text":"<p>Resample input image into the specified pixdim. It will require <code>affine</code> meta information.</p> <pre><code>from medicai.transforms import (\n    Compose,\n    Spacing,\n)\n\n# define transformations\ntransform = Compose([\n    Spacing(\n        keys=[\"image\", \"label\"], \n        pixdim=[2.0, 1.5, 1.5]\n    )\n])\n\n# load the raw data\ndata, meta = PyLoadImage(image_path=image_path, label_path=mask_path)\n\n# passing sample to medicai transform\noutput = transform(data, meta)\n\n# get transformed sample\nmedicai_image = output['image'].numpy()\nmedicai_label = output['label'].numpy()\n</code></pre>"},{"location":"transformations/manage-transformations/#orientation","title":"Orientation","text":"<p>Change the orientation of the input image to the specified one based on the provided <code>axcodes</code>. It will require <code>affine</code> meta information.</p> <pre><code>from medicai.transforms import (\n    Compose,\n    Orientation,\n)\n\n# define transformations\ntransform = Compose([\n    Orientation(\n        keys=[\"image\", \"label\"], \n        axcodes=\"RAS\"\n    )\n])\n\n# load the raw data\ndata, meta = PyLoadImage(image_path=image_path, label_path=mask_path)\n\n# passing sample to medicai transform\noutput = transform(data, meta)\n\n# get transformed sample\nmedicai_image = output['image'].numpy()\nmedicai_label = output['label'].numpy()\n</code></pre>"},{"location":"transformations/manage-transformations/#random-rotate-90","title":"Random Rotate 90","text":"<p>Rotate the input sample 90 degree at random.</p> <pre><code>from medicai.transforms import (\n    Compose,\n    RandRotate90,\n)\n\n# define transformations\ntransform = Compose([\n    RandRotate90(\n        keys=[\"image\", \"label\"], \n        prob=1.0, \n        max_k=3,\n        spatial_axes=(2,1)\n    )\n])\n\n# load the raw data\ndata, meta = PyLoadImage(image_path=image_path, label_path=mask_path)\n\n# passing sample to medicai transform\noutput = transform(data, meta)\n\n# get transformed sample\nmedicai_image = output['image'].numpy()\nmedicai_label = output['label'].numpy()\n</code></pre>"},{"location":"transformations/manage-transformations/#random-shift-intensity","title":"Random Shift Intensity","text":"<p>Randomly shift the intensity of the image by applying a randomly selected offset.</p> <pre><code>from medicai.transforms import (\n    Compose,\n    RandShiftIntensity,\n)\n\n# define transformations\ntransform = Compose([\n    RandShiftIntensity(\n        keys=[\"image\"],\n        offsets=0.10,\n        prob=1\n    )\n])\n\n# load the raw data\ndata, meta = PyLoadImage(image_path=image_path, label_path=mask_path)\n\n# passing sample to medicai transform\noutput = transform(data, meta)\n\n# get transformed sample\nmedicai_image = output['image'].numpy()\nmedicai_label = output['label'].numpy()\n</code></pre>"},{"location":"transformations/manage-transformations/#random-crop-by-positive-negative-label","title":"Random Crop By Positive Negative Label","text":"<p>Randomly crop fixed-sized regions from the image, with the center of each crop being either a foreground or background voxel based on the specified Positive-Negative Ratio. The function will return a list of arrays for all the cropped images.</p> <pre><code>from medicai.transforms import (\n    Compose,\n    RandCropByPosNegLabel,\n)\n\n# define transformations\ntransform = Compose([\n    RandCropByPosNegLabel(\n        keys=[\"image\", \"label\"], \n        spatial_size=(96, 96, 96), \n        pos=1, \n        neg=1, \n        num_samples=1,\n        image_reference_key=\"image\",\n        image_threshold=0\n    )\n])\n\n# load the raw data\ndata, meta = PyLoadImage(image_path=image_path, label_path=mask_path)\n\n# passing sample to medicai transform\noutput = transform(data, meta)\n\n# get transformed sample\nmedicai_image = output['image'].numpy()\nmedicai_label = output['label'].numpy()\n</code></pre>"},{"location":"transformations/manage-transformations/#random-spatial-crop","title":"Random Spatial Crop","text":"<p>Randomly crops a region of interest (ROI) with a specified size from the input tensors. This transform extracts a 3D spatial ROI from the tensors specified by <code>keys</code>. The size and center of the ROI can be either fixed or randomly determined within the bounds of the input tensor.</p> <pre><code>from medicai.transforms import (\n    Compose,\n    RandSpatialCrop,\n)\n\n# define transformations\ntransform = Compose([\n    RandSpatialCrop(\n        keys=[\"image\", \"label\"],\n        roi_size=(96, 96, 96),\n    )\n])\n\n# load the raw data\ndata, meta = PyLoadImage(image_path=image_path, label_path=mask_path)\n\n# passing sample to medicai transform\noutput = transform(data, meta)\n\n# get transformed sample\nmedicai_image = output['image'].numpy()\nmedicai_label = output['label'].numpy()\n</code></pre>"},{"location":"transformations/manage-transformations/#random-flip","title":"Random Flip","text":"<p>Randomly flips tensors along specified spatial axes with a given probability. This transformation randomly decides whether to flip the input tensors based on the provided probability. If the decision is to flip, it reverses the tensor elements along the specified spatial axes.</p> <pre><code>from medicai.transforms import (\n    Compose,\n    RandFlip,\n)\n\n# define transformations\ntransform = Compose([\n    RandFlip(\n        keys=[\"image\", \"label\"],\n        spatial_axis=[0],\n        prob=1.0,\n    ),\n    RandFlip(\n        keys=[\"image\", \"label\"],\n        spatial_axis=[1],\n        prob=1.0,\n    ),\n    RandFlip(\n        keys=[\"image\", \"label\"],\n        spatial_axis=[2],\n        prob=1.0,\n    )\n])\n\n# load the raw data\ndata, meta = PyLoadImage(image_path=image_path, label_path=mask_path)\n\n# passing sample to medicai transform\noutput = transform(data, meta)\n\n# get transformed sample\nmedicai_image = output['image'].numpy()\nmedicai_label = output['label'].numpy()\n</code></pre>"}]}